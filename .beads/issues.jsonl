{"id":"mlx-tts-1qu","title":"Add logging to tts-notify.py","description":"## Decision\nLog file location: logs/tts-notify.log\n\n## Rationale\n- Fully contained within plugin directory (no external paths)\n- Conventional logs/ directory name\n- Discoverable for debugging\n- Filename matches script name (tts-notify.py â†’ tts-notify.log)\n\n## Implementation\n1. Create logs/ directory if not exists\n2. Add file logging to scripts/tts-notify.py\n3. Log hook invocations, threshold checks, TTS events\n\n## Already done\n- .gitignore updated to exclude logs/","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-24T15:42:07.067722-08:00","updated_at":"2025-12-24T15:55:01.574085-08:00","closed_at":"2025-12-24T15:55:01.574085-08:00","close_reason":"Implemented file logging to logs/tts-notify.log. Logs hook invocations, threshold checks, TTS engine selection, and errors."}
{"id":"mlx-tts-5x9","title":"Implement TTS background daemon","description":"## Objective\nImplement TTS server lifecycle management with mlx_audio.server for sub-second response times.\n\n## ğŸ”´ğŸŸ¢ TDD Workflow - Use `/tdd-workflows:tdd-orchestrator` Skill\n\n**IMPORTANT**: This task follows strict TDD discipline. Use the TDD orchestrator skill to guide red-green-refactor cycles.\n\n### How to Execute\n1. Start with: `/tdd-workflows:tdd-orchestrator`\n2. Follow the red-green-refactor discipline the skill enforces\n3. Never write implementation before failing tests\n\n---\n\n## Architecture: Built-in mlx_audio.server (Option B)\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     HTTP POST        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ tts-notify  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚ mlx_audio.server â”‚\nâ”‚   (hook)    â”‚  localhost:21099     â”‚  (keeps model    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  /v1/audio/speech    â”‚     warm)        â”‚\n                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Configuration\n- **Port**: 21099 (avoiding common ports like 9000)\n- **Idle timeout**: 15 min (configurable via TTS_IDLE_TIMEOUT env)\n- **Model**: mlx-community/chatterbox-turbo-fp16 (~4GB)\n\n## Lifecycle\n\n```\nHook fires â†’ Server alive? â”€â”€Yesâ”€â”€â–¶ Send TTS request\n                  â”‚\n                  No\n                  â†“\n            Start server\n            Wait for ready (~10s cold)\n            Preload model via POST /v1/models\n            Send TTS request\n\nServer: Auto-shutdown after 15 min idle\nUser: Can run /tts-stop to reclaim memory immediately\n```\n\n## Implementation Phases (TDD Approach)\n\n### Phase 1: Server Health Check\n**ğŸ”´ Red Phase**\n1. Create `tests/test_server_lifecycle.py`\n2. Test `is_server_alive(port=21099)` function\n3. Test `wait_for_ready(port, timeout)` function\n4. All tests FAIL\n\n**ğŸŸ¢ Green Phase**\n1. Implement health check in `mlx_server_utils.py`\n2. Tests PASS\n\n### Phase 2: Auto-Start Logic\n**ğŸ”´ Red Phase**\n1. Test `ensure_server_running()` starts server if not alive\n2. Test server preloads model after start\n3. All tests FAIL\n\n**ğŸŸ¢ Green Phase**\n1. Implement auto-start with subprocess.Popen\n2. Add model preload via `POST /v1/models`\n3. Tests PASS\n\n### Phase 3: TTS via HTTP\n**ğŸ”´ Red Phase**\n1. Test `speak_mlx_http(text, speed)` sends to server\n2. Test fallback to direct API if server fails\n3. All tests FAIL\n\n**ğŸŸ¢ Green Phase**\n1. Implement HTTP client for `/v1/audio/speech`\n2. Add fallback logic\n3. Tests PASS\n\n### Phase 4: Stop Utility \u0026 Skill\n**ğŸ”µ Refactor Phase**\n1. Create `scripts/tts-stop.sh`\n2. Register `/tts-stop` skill in hooks.json\n3. Optional: `/tts-status` skill\n\n## Key Code\n\n**Server startup:**\n```python\ndef ensure_server_running():\n    if is_server_alive(21099):\n        return\n    \n    subprocess.Popen([\n        sys.executable, \"-m\", \"mlx_audio.server\",\n        \"--port\", \"21099\",\n        \"--idle-timeout\", str(TTS_IDLE_TIMEOUT)\n    ])\n    wait_for_ready(21099, timeout=30)\n    preload_model(\"mlx-community/chatterbox-turbo-fp16\")\n```\n\n**TTS request:**\n```python\ndef speak_mlx_http(text: str, speed: float = 1.6):\n    ensure_server_running()\n    response = requests.post(\n        \"http://localhost:21099/v1/audio/speech\",\n        json={\"text\": text, \"speed\": speed, \"play\": True}\n    )\n```\n\n## Test Cases\n\n```python\ndef test_is_server_alive_returns_false_when_not_running():\n    assert is_server_alive(21099) is False\n\ndef test_ensure_server_starts_when_not_running():\n    ensure_server_running()\n    assert is_server_alive(21099) is True\n\ndef test_speak_mlx_http_warm_latency():\n    ensure_server_running()\n    start = time.time()\n    speak_mlx_http(\"Quick test\")\n    assert time.time() - start \u003c 1.0\n```\n\n## Success Metrics\n- [ ] Warm latency \u003c1s\n- [ ] Cold start \u003c15s (server + model load)\n- [ ] Auto-shutdown after 15 min idle\n- [ ] /tts-stop reclaims memory immediately\n- [ ] Graceful fallback if server fails\n\n## Blocked By\n- mlx-tts-dno (need direct API as fallback)","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-24T15:51:54.004142-08:00","updated_at":"2025-12-24T16:25:38.943226-08:00","dependencies":[{"issue_id":"mlx-tts-5x9","depends_on_id":"mlx-tts-dno","type":"blocks","created_at":"2025-12-24T15:52:10.264515-08:00","created_by":"daemon"}]}
{"id":"mlx-tts-df0","title":"Migrate to direct mlx_audio Python API","description":"## Goal\nReplace subprocess-based MLX TTS with HTTP API to mlx_audio.server. Enable model caching via persistent server for sub-second response times.\n\n## ğŸ”´ğŸŸ¢ TDD Workflow Required\n\n**All child tasks use strict TDD discipline with `/tdd-workflows:tdd-orchestrator` skill.**\n\nEach implementation phase follows red-green-refactor:\n1. **ğŸ”´ Red**: Write failing tests first\n2. **ğŸŸ¢ Green**: Minimal implementation to pass\n3. **ğŸ”µ Refactor**: Clean up without breaking tests\n\n---\n\n## Architecture Decision: Built-in Server (Option B)\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     HTTP POST        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ tts-notify  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚ mlx_audio.server â”‚\nâ”‚   (hook)    â”‚  localhost:21099     â”‚  (model cached)  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  /v1/audio/speech    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                            â–²\nUser control:                               â”‚\n  /tts-start  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n  /tts-stop   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n  /tts-status â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n**Why Option B over custom daemon:**\n- Less code to maintain\n- Server already battle-tested\n- Model management endpoints built-in\n- Idle timeout support\n\n## Configuration\n- **Port**: 21099\n- **Idle timeout**: 15 min\n- **Model**: mlx-community/chatterbox-turbo-fp16\n\n## Current Problem\nEach TTS call forks a process and loads ~4GB model from scratch. Latency is 5-10+ seconds.\n\n## Target State\nServer auto-starts on first TTS, stays warm for 15 min. Subsequent calls \u003c1s.\nUser can manually control via /tts-start, /tts-stop, /tts-status skills.\n\n## Version History\n- **v0.1.0**: Subprocess-based implementation âœ… (tagged)\n- **v0.2.0**: Direct Python API (in progress)\n- **v0.3.0**: HTTP to mlx_audio.server + control skills (planned)\n\n## Implementation Roadmap\n\n### âœ… Phase 1: Research (mlx-tts-sqn) - COMPLETE\n- Documented mlx_audio Python API\n- Confirmed model reuse pattern works\n\n### ğŸ”² Phase 2: Direct API (mlx-tts-dno) - READY\n- Replace subprocess with direct API calls\n- Serves as fallback when server unavailable\n- **Use `/tdd-workflows:tdd-orchestrator`**\n\n### ğŸ”² Phase 3: Server Integration (mlx-tts-5x9) - BLOCKED\n- Auto-start mlx_audio.server on port 21099\n- 15 min idle timeout\n- **Use `/tdd-workflows:tdd-orchestrator`**\n\n### ğŸ”² Phase 4: Control Skills (mlx-tts-lc0) - BLOCKED\n- /tts-start - manual server start + model preload\n- /tts-stop - immediate shutdown, reclaim memory\n- /tts-status - show server state, memory, idle time\n- **Use `/tdd-workflows:tdd-orchestrator`**\n\n## Success Metrics\n- Warm latency: \u003c1s\n- Cold start: \u003c15s (acceptable for first call)\n- Memory: Auto-free after 15 min idle\n- User control: /tts-start, /tts-stop, /tts-status\n\n## Dependencies\n```\nmlx-tts-sqn âœ… â†’ mlx-tts-dno ğŸ”² â†’ mlx-tts-5x9 ğŸ”² â†’ mlx-tts-lc0 ğŸ”²\n```","status":"open","priority":2,"issue_type":"epic","created_at":"2025-12-24T15:51:13.414815-08:00","updated_at":"2025-12-24T16:25:43.972604-08:00","dependencies":[{"issue_id":"mlx-tts-df0","depends_on_id":"mlx-tts-5x9","type":"blocks","created_at":"2025-12-24T15:52:10.341169-08:00","created_by":"daemon"}]}
{"id":"mlx-tts-dno","title":"Implement direct mlx_audio API calls","description":"## Objective\nReplace subprocess-based MLX TTS with direct Python API calls.\n\n## Prerequisites\n- mlx-tts-sqn (Research) - COMPLETE\n\n## ğŸ”´ğŸŸ¢ TDD Workflow - Use `/tdd-workflows:tdd-orchestrator` Skill\n\n**IMPORTANT**: This task follows strict TDD discipline. Use the TDD orchestrator skill to guide red-green-refactor cycles.\n\n### How to Execute\n1. Start with: `/tdd-workflows:tdd-orchestrator`\n2. Follow the red-green-refactor discipline the skill enforces\n3. Never write implementation before failing tests\n\n---\n\n## Implementation Phases (TDD Approach)\n\n### Phase 1: Test Infrastructure\n**ğŸ”´ Red Phase** - Write failing tests first\n1. Create `tests/test_mlx_tts.py` with pytest fixtures\n2. Write tests for model loading (mock mlx_audio imports)\n3. Write tests for audio generation with preloaded model\n4. Write tests for play=True behavior\n5. Run tests - all should FAIL (no implementation yet)\n\n### Phase 2: Core API Integration  \n**ğŸŸ¢ Green Phase** - Minimal implementation to pass tests\n1. Create `mlx_tts_core.py` module with:\n   - `load_tts_model()` - wraps load_model with error handling\n   - `generate_speech()` - wraps generate_audio with our defaults\n2. Update `tts-notify.py` to import and use new module\n3. Run tests - all should PASS\n\n### Phase 3: Refactor \u0026 Optimize\n**ğŸ”µ Refactor Phase** - Clean up without breaking tests\n1. Add model caching at module level\n2. Add lazy loading with `@functools.lru_cache`\n3. Add timeout handling for generation\n4. Run tests - all should still PASS\n\n## Key Code Changes\n\n**Before (subprocess):**\n```python\nsubprocess.run([\n    sys.executable, \"-m\", \"mlx_audio.tts.generate\",\n    \"--model\", MLX_MODEL,\n    \"--text\", message,\n    ...\n])\n```\n\n**After (direct API):**\n```python\nfrom mlx_audio.tts.utils import load_model\nfrom mlx_audio.tts.generate import generate_audio\n\n_model = None\n\ndef get_model():\n    global _model\n    if _model is None:\n        _model = load_model(\"mlx-community/chatterbox-turbo-fp16\")\n    return _model\n\ndef speak_mlx(message: str):\n    generate_audio(\n        text=message,\n        model=get_model(),\n        ref_audio=MLX_VOICE_REF,\n        ref_text=\".\",\n        speed=MLX_SPEED,\n        play=True,\n        verbose=False,\n    )\n```\n\n## Test Cases\n\n### Unit Tests\n```python\ndef test_load_tts_model_returns_valid_model():\n    model = load_tts_model()\n    assert hasattr(model, 'sample_rate')\n\ndef test_generate_speech_with_preloaded_model():\n    model = load_tts_model()\n    # Should not raise, should play audio\n    generate_speech(\"Test\", model=model, play=True)\n\ndef test_model_reuse_no_reload():\n    m1 = get_model()\n    m2 = get_model()\n    assert m1 is m2  # Same instance\n```\n\n### Integration Tests  \n```python\ndef test_warm_latency_under_1_second():\n    model = get_model()  # Cold load\n    start = time.time()\n    generate_speech(\"Quick test\", model=model, play=False)\n    assert time.time() - start \u003c 1.0\n```\n\n## Success Metrics\n- [ ] All unit tests pass\n- [ ] Warm latency \u003c1s\n- [ ] No subprocess calls for MLX TTS\n- [ ] Graceful fallback to macOS say on failure\n\n## Blocked By\n- mlx-tts-sqn (Research) âœ… COMPLETE","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-24T15:51:53.806082-08:00","updated_at":"2025-12-24T16:23:23.381408-08:00","dependencies":[{"issue_id":"mlx-tts-dno","depends_on_id":"mlx-tts-sqn","type":"blocks","created_at":"2025-12-24T15:52:10.187281-08:00","created_by":"daemon"}]}
{"id":"mlx-tts-lc0","title":"Add TTS control skills (start/stop/status)","description":"## Objective\nAdd Claude Code slash commands for manual TTS server lifecycle control.\nPackage as part of the plugin for easy install.\n\n## ğŸ”´ğŸŸ¢ TDD Workflow - Use `/tdd-workflows:tdd-orchestrator` Skill\n\n**IMPORTANT**: This task follows strict TDD discipline. Use the TDD orchestrator skill to guide red-green-refactor cycles.\n\n### How to Execute\n1. Start with: `/tdd-workflows:tdd-orchestrator`\n2. Follow the red-green-refactor discipline the skill enforces\n3. Write shell script tests (bats) before implementation\n\n---\n\n## Research Finding\nPlugins support **slash commands** in `commands/` directory:\n- User-invoked via `/plugin-name:command` or just `/command`\n- Defined as Markdown files with YAML frontmatter\n- Scripts referenced via `${CLAUDE_PLUGIN_ROOT}`\n\n## Plugin Structure\n\n```\nclaude-mlx-tts/\nâ”œâ”€â”€ .claude-plugin/\nâ”‚   â””â”€â”€ plugin.json          â† add \"commands\": \"./commands/\"\nâ”œâ”€â”€ commands/                 â† NEW\nâ”‚   â”œâ”€â”€ tts-start.md\nâ”‚   â”œâ”€â”€ tts-stop.md\nâ”‚   â””â”€â”€ tts-status.md\nâ”œâ”€â”€ hooks/\nâ”‚   â””â”€â”€ hooks.json\nâ”œâ”€â”€ tests/                    â† NEW (TDD)\nâ”‚   â””â”€â”€ test_tts_commands.bats\nâ””â”€â”€ scripts/\n    â”œâ”€â”€ tts-notify.py\n    â”œâ”€â”€ tts-start.sh         â† NEW\n    â”œâ”€â”€ tts-stop.sh          â† NEW\n    â””â”€â”€ tts-status.sh        â† NEW\n```\n\n## Implementation Phases (TDD Approach)\n\n### Phase 1: Test Infrastructure\n**ğŸ”´ Red Phase**\n1. Create `tests/test_tts_commands.bats`\n2. Write tests for tts-start.sh behavior\n3. Write tests for tts-stop.sh behavior  \n4. Write tests for tts-status.sh behavior\n5. All tests FAIL (scripts don't exist)\n\n### Phase 2: Script Implementation\n**ğŸŸ¢ Green Phase**\n1. Implement tts-start.sh\n2. Implement tts-stop.sh\n3. Implement tts-status.sh\n4. Tests PASS\n\n### Phase 3: Plugin Integration\n**ğŸ”µ Refactor Phase**\n1. Create command markdown files\n2. Update plugin.json\n3. Verify commands work via Claude\n\n---\n\n## Commands to Implement\n\n### /tts-start\n```markdown\n---\nname: tts-start\ndescription: Start TTS server and preload MLX model (~4GB, takes ~10s)\n---\n\nStart the MLX TTS server on port 21099 and preload the Chatterbox model.\n\n\\`\\`\\`bash\n${CLAUDE_PLUGIN_ROOT}/scripts/tts-start.sh\n\\`\\`\\`\n```\n\n**Script logic:**\n```bash\n#\\!/bin/bash\nPORT=21099\nif lsof -i :$PORT \u003e/dev/null 2\u003e\u00261; then\n    echo \"TTS server already running on port $PORT\"\n    exit 0\nfi\necho \"Starting TTS server on port $PORT...\"\nmlx_audio.server --port $PORT \u0026\nsleep 2\necho \"Preloading model...\"\ncurl -X POST http://localhost:$PORT/v1/models \\\n  -d '{\"model\": \"mlx-community/chatterbox-turbo-fp16\"}'\necho \"Server ready.\"\n```\n\n### /tts-stop\n```markdown\n---\nname: tts-stop\ndescription: Stop TTS server and reclaim ~4GB memory\n---\n\nStop the MLX TTS server and free memory.\n\n\\`\\`\\`bash\n${CLAUDE_PLUGIN_ROOT}/scripts/tts-stop.sh\n\\`\\`\\`\n```\n\n**Script logic:**\n```bash\n#\\!/bin/bash\nPORT=21099\nPID=$(lsof -ti :$PORT)\nif [ -z \"$PID\" ]; then\n    echo \"TTS server not running\"\n    exit 0\nfi\nkill $PID\necho \"TTS server stopped. Memory reclaimed.\"\n```\n\n### /tts-status\n```markdown\n---\nname: tts-status\ndescription: Check TTS server status, model loaded, memory usage\n---\n\nShow TTS server status.\n\n\\`\\`\\`bash\n${CLAUDE_PLUGIN_ROOT}/scripts/tts-status.sh\n\\`\\`\\`\n```\n\n**Script logic:**\n```bash\n#\\!/bin/bash\nPORT=21099\nif \\! lsof -i :$PORT \u003e/dev/null 2\u003e\u00261; then\n    echo \"TTS Server: not running\"\n    echo \"Hint: Will auto-start on next TTS, or run /tts-start\"\n    exit 0\nfi\n\necho \"TTS Server: running on port $PORT\"\n# Query server for model info\nMODELS=$(curl -s http://localhost:$PORT/v1/models)\necho \"Models: $MODELS\"\n# Memory usage\nPID=$(lsof -ti :$PORT)\nMEM=$(ps -o rss= -p $PID | awk '{print $1/1024 \"MB\"}')\necho \"Memory: $MEM\"\n```\n\n## plugin.json Update\n\n```json\n{\n  \"name\": \"claude-mlx-tts\",\n  \"description\": \"Voice-cloned TTS notifications using MLX Chatterbox Turbo\",\n  \"version\": \"1.2.0\",\n  \"author\": {\"name\": \"aperepel\"},\n  \"commands\": \"./commands/\",\n  \"hooks\": \"./hooks/hooks.json\"\n}\n```\n\n## Test Cases (Bats)\n```bash\n@test \"tts-start when not running starts server\" {\n    run ./scripts/tts-start.sh\n    [ \"$status\" -eq 0 ]\n    [[ \"$output\" =~ \"Server ready\" ]]\n}\n\n@test \"tts-start when running says already running\" {\n    # Start first\n    ./scripts/tts-start.sh\n    run ./scripts/tts-start.sh\n    [[ \"$output\" =~ \"already running\" ]]\n}\n\n@test \"tts-stop when running stops server\" {\n    ./scripts/tts-start.sh\n    run ./scripts/tts-stop.sh\n    [ \"$status\" -eq 0 ]\n    [[ \"$output\" =~ \"stopped\" ]]\n}\n```\n\n## Success Metrics\n- [ ] All bats tests pass\n- [ ] Commands packaged in plugin (easy install)\n- [ ] All three commands work from Claude prompt\n- [ ] No manual PATH setup required\n\n## Depends On\n- mlx-tts-5x9 (server integration)","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-24T16:10:58.266754-08:00","updated_at":"2025-12-24T16:25:43.78237-08:00","dependencies":[{"issue_id":"mlx-tts-lc0","depends_on_id":"mlx-tts-5x9","type":"blocks","created_at":"2025-12-24T16:11:07.607257-08:00","created_by":"daemon"}]}
{"id":"mlx-tts-sqn","title":"Research mlx_audio Python API","description":"## Objective\nDocument the direct Python API for mlx_audio TTS generation.\n\n## Research Findings (Completed)\n\n### Key API Components\n\n**1. Model Loading** (`mlx_audio.tts.utils`)\n```python\nfrom mlx_audio.tts.utils import load_model\n\n# Returns nn.Module, auto-downloads from HuggingFace if needed\nmodel = load_model(model_path=\"mlx-community/chatterbox-turbo-fp16\")\n```\n- `lazy=False` (default): Loads all weights into memory immediately\n- `lazy=True`: Deferred loading, params load on first use\n- Auto-caches via HuggingFace `snapshot_download()`\n\n**2. Audio Generation** (`mlx_audio.tts.generate`)\n```python\nfrom mlx_audio.tts.generate import generate_audio\n\ngenerate_audio(\n    text=\"Hello world\",\n    model=model,              # Pass pre-loaded model OR string path\n    ref_audio=\"voice.wav\",    # Voice reference for cloning\n    ref_text=\".\",             # Reference transcript (optional)\n    speed=1.6,                # 0.5x to 2.0x\n    play=True,                # Direct playback via AudioPlayer\n    file_prefix=\"output\",     # File output (optional)\n    verbose=False,\n)\n```\n- Returns `None`, outputs via file or AudioPlayer\n- Generator yields objects with `.audio`, `.sample_rate`, `.audio_duration`\n\n**3. Model Reuse Pattern**\n```python\n# Load once, reuse many times\nmodel = load_model(\"mlx-community/chatterbox-turbo-fp16\")\n\n# Subsequent calls skip loading\ngenerate_audio(text=\"First\", model=model, play=True)\ngenerate_audio(text=\"Second\", model=model, play=True)  # No reload\n```\n\n**4. Built-in Server** (`mlx_audio.server`)\n```bash\nmlx_audio.server --host 0.0.0.0 --port 9000\n```\n- REST API: `POST /v1/models` to preload, `GET /v1/models` to list\n- Can keep models warm via model management endpoints\n\n### Memory \u0026 Performance\n- Model size: ~4GB for fp16, ~1GB for 4-bit quantized\n- Cold start: 5-10s (model load + first inference)\n- Warm inference: \u003c1s (model already in memory)\n- `MAX_FILE_SIZE_GB = 5` limit per weight file\n\n### TDD Acceptance Criteria\n\n**Unit Tests (test_mlx_api.py)**\n- [ ] `test_load_model_returns_nn_module`: load_model returns valid nn.Module\n- [ ] `test_load_model_caches`: Second load uses cache (no re-download)\n- [ ] `test_generate_audio_with_preloaded_model`: Accepts model object\n- [ ] `test_generate_audio_with_play_flag`: play=True triggers AudioPlayer\n- [ ] `test_generate_audio_speed_parameter`: speed affects output duration\n\n**Integration Tests (test_mlx_integration.py)**\n- [ ] `test_model_reuse_performance`: Second call faster than first\n- [ ] `test_warm_model_latency_under_1s`: Warm inference \u003c1s\n- [ ] `test_fallback_to_say_on_mlx_failure`: Graceful degradation\n\n**Sources**\n- [mlx-audio GitHub](https://github.com/Blaizzy/mlx-audio)\n- [Chatterbox MLX model](https://huggingface.co/mlx-community/Chatterbox-TTS-fp16)\n- generate.py source: function signature with model reuse pattern\n\n## Status: Research Complete\nReady for implementation in mlx-tts-dno","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-24T15:51:28.00252-08:00","updated_at":"2025-12-24T16:03:18.51917-08:00","closed_at":"2025-12-24T16:03:18.51917-08:00","close_reason":"Research complete - documented mlx_audio Python API with TDD acceptance criteria"}
